# -*- coding: utf-8 -*-
"""Appipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UvhRgXbZ9WSgt60EcQxougUu28Xsntbo
"""

import os
import io
import json
import tempfile
from typing import List, Tuple, Optional

import numpy as np
import pandas as pd
from PIL import Image
import streamlit as st
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torchvision.transforms as T
from torchvision import models
from torchvision.models import EfficientNet_B0_Weights
from pycocotools import mask as maskUtils
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix
import seaborn as sns

# ------------------------------
# Helpers: model loading & preprocessing
# ------------------------------
@st.cache_resource
def load_model(weights_bytes: Optional[bytes], device: str = "cpu"):
    """
    Build EfficientNet-B0, replace classifier and load weights bytes (if provided).
    weights_bytes: raw bytes of .pth file (uploaded)
    device: "cpu" or "cuda"
    """
    dev = torch.device(device)
    # Load model with pretrained weights for normalization defaults (weights arg optional)
    model = models.efficientnet_b0(weights=EfficientNet_B0_Weights.DEFAULT)
    in_features = model.classifier[1].in_features
    model.classifier[1] = nn.Linear(in_features, 2)
    model.to(dev)
    if weights_bytes is not None:
        # load from bytes
        state = torch.load(io.BytesIO(weights_bytes), map_location=dev)
        # support both state_dict and full model states
        if isinstance(state, dict) and any(k.startswith("module.") for k in state.keys()):
            # strip module.
            state = {k.replace("module.", ""): v for k, v in state.items()}
        try:
            model.load_state_dict(state)
        except Exception as e:
            # maybe state is nested (like {"model_state_dict": ...})
            if "state_dict" in state:
                model.load_state_dict(state["state_dict"])
            elif "model_state_dict" in state:
                model.load_state_dict(state["model_state_dict"])
            else:
                raise e
    model.eval()
    return model

# Preprocessing transforms (same normalization as training)
def get_transforms():
    val_tf = T.Compose([
        T.Resize((224,224)),
        T.ToTensor(),
        T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
    ])
    return val_tf

# Crop to mask region (512 assumed) same behavior as your dataset code
def crop_to_mask(img: Image.Image, ann: Optional[dict]) -> Image.Image:
    w,h = img.width, img.height
    mask = np.zeros((512,512), dtype=np.uint8)
    if ann:
        seg = ann.get("segmentation")
        if seg:
            if isinstance(seg, list):
                rles = maskUtils.frPyObjects(seg,512,512)
                rle = maskUtils.merge(rles)
                mask = maskUtils.decode(rle)
            elif isinstance(seg, dict):
                mask = maskUtils.decode(seg)
    mask = (mask>0).astype(np.uint8)
    ys,xs = np.where(mask>0)
    if len(xs)==0:
        return img.resize((224,224))
    xmin,xmax,ymin,ymax = xs.min(),xs.max(),ys.min(),ys.max()
    pad = 10
    xmin,ymin = max(0,xmin-pad),max(0,ymin-pad)
    xmax,ymax = min(511,xmax+pad),min(511,ymax+pad)  # masks assumed 512x512
    # if original image isn't 512x512, scale bounding box to image dims
    # assume annotations were for 512x512 original images; map coords proportionally
    x_scale = w/512
    y_scale = h/512
    bbox = (int(xmin*x_scale), int(ymin*y_scale), int(xmax*x_scale), int(ymax*y_scale))
    crop = img.crop(bbox).resize((224,224))
    return crop

# Run prediction on a single PIL image
def predict_image(model: nn.Module, img: Image.Image, device: str = "cpu"):
    tf = get_transforms()
    x = tf(img).unsqueeze(0).to(device)
    with torch.no_grad():
        logits = model(x)
        probs = torch.softmax(logits, dim=1).cpu().numpy()[0]
        pred = int(np.argmax(probs))
    return pred, float(probs[1]), probs  # label index, probability of class 1, full probs

# Find last conv layer for gradcam (heuristic)
def find_target_layer(model):
    # For efficientnet_b0, features[-1] is a Sequential block; search inside for Conv2d
    last = None
    for name, module in model.named_modules():
        if isinstance(module, nn.Conv2d):
            last = (name, module)
    return last

# Grad-CAM implementation (simple)
def gradcam(model, input_tensor: torch.Tensor, target_class: int, device: str = "cpu"):
    """
    Returns heatmap numpy (H x W) and overlay image (PIL)
    input_tensor: single image tensor (1,C,H,W) preprocessed and on device
    """
    model.eval()
    dev = torch.device(device)
    input_tensor = input_tensor.to(dev)

    # Find target conv layer name
    target_name, _ = find_target_layer(model)
    if target_name is None:
        raise RuntimeError("No Conv2d layer found for Grad-CAM.")

    activations = {}
    gradients = {}

    def forward_hook(module, inp, out):
        activations['value'] = out.detach()

    def backward_hook(module, grad_in, grad_out):
        gradients['value'] = grad_out[0].detach()

    # attach hooks
    # get module by name
    module = dict(model.named_modules())[target_name]
    fh = module.register_forward_hook(forward_hook)
    bh = module.register_full_backward_hook(backward_hook)

    # forward
    output = model(input_tensor)
    loss = output[0, target_class]
    model.zero_grad()
    loss.backward(retain_graph=True)

    act = activations['value'].cpu().squeeze(0)  # CxHxW
    grad = gradients['value'].cpu().squeeze(0)  # CxHxW

    weights = grad.mean(dim=(1,2))  # global avg pooling over H,W
    cam = (weights.view(-1,1,1) * act).sum(dim=0).numpy()
    # relu & normalize
    cam = np.maximum(cam, 0)
    cam = cam - cam.min()
    if cam.max() > 0:
        cam = cam / cam.max()
    # resize to 224x224
    cam = (cam * 255).astype(np.uint8)
    cam_img = Image.fromarray(cam).resize((224,224), resample=Image.BILINEAR)

    # cleanup hooks
    fh.remove(); bh.remove()

    return np.array(cam_img)  # 224x224 grayscale

# Overlay heatmap on original PIL image
def overlay_heatmap(pil_img: Image.Image, heatmap: np.ndarray, alpha: float = 0.5):
    import matplotlib.cm as cm
    heatmap = heatmap.astype(np.float32)/255.0
    cmap = cm.jet(heatmap)[:,:,:3]  # RGB
    heatmap_img = Image.fromarray((cmap*255).astype(np.uint8)).convert("RGBA").resize(pil_img.size)
    base = pil_img.convert("RGBA").resize(pil_img.size)
    blended = Image.blend(base, heatmap_img, alpha=alpha)
    return blended

# ------------------------------
# Streamlit UI
# ------------------------------
st.set_page_config(page_title="EfficientNet-B0 — Receptive vs Non-Receptive", layout="wide")
st.title("EfficientNet-B0 — Receptive vs Non-Receptive — Inference & Explainability")

# Sidebar: model & runtime options
st.sidebar.header("Model / Runtime")
uploaded_weights = st.sidebar.file_uploader("Upload fine-tuned model (.pth) (optional)", type=["pth","pt"])
use_gpu = st.sidebar.checkbox("Use GPU (cuda)", value=False)
device = "cuda" if (use_gpu and torch.cuda.is_available()) else "cpu"
st.sidebar.write("Device:", device)
st.sidebar.markdown("---")
uploaded_coco = st.sidebar.file_uploader("Upload COCO annotations JSON (optional, for cropping)", type=["json"])
st.sidebar.markdown("If you upload COCO-style JSON, the app will use segmentation mask to crop the image region (same crop logic as training).")
st.sidebar.markdown("---")
st.sidebar.write("Classes: 0 = non-receptive, 1 = receptive")

# Load model (cache)
weights_bytes = None
if uploaded_weights is not None:
    weights_bytes = uploaded_weights.read()
    st.sidebar.success("Model file uploaded")

with st.spinner("Loading model..."):
    model = load_model(weights_bytes, device=device)

# Main UI: image upload / batch folder
st.header("Inference")
col1, col2 = st.columns([1,2])

with col1:
    st.subheader("Upload single image or multiple images")
    uploaded_imgs = st.file_uploader("Upload image(s)", type=["jpg","jpeg","png","tif","tiff"], accept_multiple_files=True)
    crop_by_mask = st.checkbox("Crop by COCO mask (requires annotations JSON)", value=False)
    run_button = st.button("Run Inference")

with col2:
    st.subheader("Results")
    result_placeholder = st.container()

# Inference & display
def run_inference_on_files(files):
    records = []
    coco_anns = {}
    img_meta = {}
    if uploaded_coco is not None:
        try:
            ann_json = json.load(uploaded_coco)
            img_meta = {img["file_name"]: img for img in ann_json.get("images", [])}
            anns_by_img = {a["image_id"]: a for a in ann_json.get("annotations", [])}
            # create fn->ann mapping (similar to dataset)
            coco_anns = {img_meta[a["image_id"]]["file_name"]: a for a in anns_by_img.values() if img_meta.get(a["image_id"])}
        except Exception as e:
            st.sidebar.error("Failed to parse COCO JSON: " + str(e))
            coco_anns = {}

    for f in files:
        try:
            img = Image.open(f).convert("RGB")
        except Exception as e:
            st.error(f"Failed to open {f.name}: {e}")
            continue

        ann = None
        if crop_by_mask and uploaded_coco is not None:
            ann = coco_anns.get(f.name)
            img_proc = crop_to_mask(img, ann)
        else:
            img_proc = img.resize((224,224))

        pred, prob1, probs = predict_image(model, img_proc, device=device)
        label = "receptive" if pred==1 else "non-receptive"

        # Grad-CAM (optional): compute on preprocessed tensor
        tf = get_transforms()
        input_tensor = tf(img_proc).unsqueeze(0)
        try:
            heat = gradcam(model, input_tensor, target_class=pred, device=device)
            overlay = overlay_heatmap(img_proc, heat, alpha=0.45)
        except Exception as e:
            heat = None
            overlay = None

        records.append({
            "filename": f.name,
            "pred_label": label,
            "prob_receptive": prob1,
            "probs": probs.tolist()
        })
        yield f.name, img, img_proc, label, prob1, overlay

# If user hits run
if run_button and uploaded_imgs:
    rows = []
    with result_placeholder:
        st.info(f"Running inference on {len(uploaded_imgs)} image(s)...")
        cols = st.columns(2)
        idx = 0
        preds = []
        for fname, orig, proc, label, prob1, overlay in run_inference_on_files(uploaded_imgs):
            c = cols[idx % 2]
            with c:
                st.markdown(f"**{fname}** — Pred: **{label}** — Prob(receptive)={prob1:.3f}")
                st.image(orig, caption=f"Original: {fname}", use_column_width=True)
                st.write("Preprocessed crop / resize:")
                st.image(proc, width=300)
                # probability bar
                fig, ax = plt.subplots(figsize=(4,0.6))
                ax.barh(["non-receptive","receptive"], [1 - prob1, prob1])
                ax.set_xlim(0,1)
                ax.set_xlabel("Probability")
                st.pyplot(fig)
                if overlay is not None:
                    st.write("Grad-CAM overlay:")
                    st.image(overlay, width=300)
            preds.append({"filename": fname, "pred_label": label, "prob_receptive": prob1})
            idx += 1

        # summary metrics if ground truth uploaded via a CSV mapping file_name->label
        st.markdown("---")
        st.subheader("Download predictions")
        df_preds = pd.DataFrame(preds)
        st.dataframe(df_preds)
        csv = df_preds.to_csv(index=False).encode()
        st.download_button("Download predictions CSV", csv, file_name="predictions.csv", mime="text/csv")

# Optional: allow evaluating a folder with ground truth CSV
st.markdown("---")
st.subheader("Batch Evaluation (optional)")

with st.expander("Evaluate on ZIP of images + CSV ground truth"):
    uploaded_zip = st.file_uploader("Upload a ZIP of images (optional)", type=["zip"])
    gt_csv = st.file_uploader("Upload CSV with columns [file_name,label] where label is 0/1 or 'non-receptive'/'receptive' (optional)", type=["csv"])
    run_eval = st.button("Run Batch Evaluation")
    if run_eval:
        if not uploaded_zip or not gt_csv:
            st.error("Please upload both a ZIP of images and the ground truth CSV to run batch evaluation.")
        else:
            import zipfile, tempfile
            zf = zipfile.ZipFile(uploaded_zip)
            tmpdir = tempfile.mkdtemp()
            zf.extractall(tmpdir)
            gt_df = pd.read_csv(gt_csv)
            # map textual labels
            if "label" in gt_df.columns and gt_df["label"].dtype == object:
                gt_df["label_mapped"] = gt_df["label"].map({"non-receptive":0,"receptive":1}).fillna(gt_df["label"])
            else:
                gt_df["label_mapped"] = gt_df["label"]
            preds = []
            files_to_run = []
            for _, row in gt_df.iterrows():
                fname = row["file_name"]
                fpath = os.path.join(tmpdir, fname)
                if os.path.exists(fpath):
                    files_to_run.append(open(fpath, "rb"))
                else:
                    st.warning(f"File not found in zip: {fname}")
            # run inference
            y_true, y_pred, y_prob = [], [], []
            for fname, orig, proc, label, prob1, overlay in run_inference_on_files(files_to_run):
                y_prob.append(prob1)
                y_pred.append(1 if label=="receptive" else 0)
                true_row = gt_df[gt_df["file_name"]==fname]
                if not true_row.empty:
                    y_true.append(int(true_row.iloc[0]["label_mapped"]))
                else:
                    y_true.append(0)  # fallback
            # compute metrics
            acc = accuracy_score(y_true, y_pred)
            f1 = f1_score(y_true, y_pred)
            prec = precision_score(y_true, y_pred)
            rec = recall_score(y_true, y_pred)
            try:
                auc = roc_auc_score(y_true, y_prob)
            except Exception:
                auc = np.nan
            st.success("Batch evaluation done.")
            st.write(f"Accuracy: {acc:.3f}")
            st.write(f"F1-score: {f1:.3f}")
            st.write(f"Precision: {prec:.3f}")
            st.write(f"Recall: {rec:.3f}")
            st.write(f"AUC: {auc if not np.isnan(auc) else 'N/A'}")

            cm = confusion_matrix(y_true, y_pred)
            fig, ax = plt.subplots(figsize=(4,3))
            sns.heatmap(cm, annot=True, fmt="d", ax=ax, cmap="Blues", xticklabels=["Non-Receptive","Receptive"], yticklabels=["Non-Receptive","Receptive"])
            ax.set_xlabel("Predicted"); ax.set_ylabel("True")
            st.pyplot(fig)

st.markdown("---")
st.caption("Notes: \n• If your model is large and you want GPU inference, run this app on a machine with CUDA and toggle 'Use GPU'.\n• If you have a remote .pth on Google Drive, download it and upload here or pass its local path and modify `load_model` accordingly.\n• Grad-CAM searches for the last Conv2d layer automatically; results are heuristic but usually informative for EfficientNet-B0.")